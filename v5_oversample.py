# -*- coding: utf-8 -*-
"""Stat653_project_v5_Oversample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_WYD9LWr_8Gx0ldJcgxljQPPM58iLjyl

# Sentimental Analysis
  # â€“ Word Embeddings (word2Vec, Glove, fasttest)
-	Oversampling data (ie to correct label imbalance by oversample method), for easier access to codes result please use below Colab link:
https://colab.research.google.com/drive/1_WYD9LWr_8Gx0ldJcgxljQPPM58iLjyl?usp=sharing

# Part 1 Data Scraping
"""

!pip install pandas
!pip install os
!pip install selenium
!pip install pandas
!pip install fake_useragent
!pip install keras

from sklearn.metrics import accuracy_score, roc_curve, auc, f1_score

"""Data Scraping: # Takes about 1 hr, rerun only if needed. Comment out for now"""

# # Takes about 1 hr, rerun only if needed
# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# import pandas as pd
# import time
# # line 7- 14 is for letting selenium chrome webdriver to run on Colab
# from fake_useragent import UserAgent

# import re

# def modify_xpath1(xpath):
#     # Define a regex pattern to match both /DIV[1]/ and /DIV[2]/
#     pattern = r'/DIV\[(1|2)\]/'

#     # Replace both patterns with an empty string in the XPath
#     modified_xpath1 = re.sub(pattern, '/DIV/', xpath)
#     modified_xpath1 = modified_xpath1.replace("(", "=")
#     modified_xpath1 = modified_xpath1.replace(")", "]")
#     modified_xpath1 = modified_xpath1.lower()

#     return modified_xpath1

# ua = UserAgent()
# userAgent = ua.chrome
# chrome_options = webdriver.ChromeOptions()
# chrome_options.add_argument('--headless')
# chrome_options.add_argument('--no-sandbox')
# chrome_options.add_argument('--disable-dev-shm-usage')
# chrome_options.add_argument(f'user-agent={userAgent}')
# driver = webdriver.Chrome(options=chrome_options)
# # create 2nd webdriver instance
# driver1 = webdriver.Chrome(options=chrome_options)
# driver2 = webdriver.Chrome(options=chrome_options)
# #//*[@id="eng-abstract"]/p/text()

# reviews_data = []
# recomend_data = []
# for page_number in range(1, 2):
#     # Construct the URL for the current page
#     url = f"https://www.airlinequality.com/airline-reviews/american-airlines/page/{page_number}/"


#     # url = 'https://www.airlinequality.com/airline-reviews/american-airlines/page/1/'
#     driver.get(url)
#     # e1 = driver.find_element(By.XPATH,'//*[@id="anchor892472"]/div/div[1]').text
#     # Example of partial matching using contains()
#     elements = driver.find_elements(By.XPATH,"//*[contains(@id, 'anchor')]/div/div[1]")

#     id_numbers = []
#     for element in elements:
#         # Get the id attribute of the element
#         id_attribute = element.get_attribute("id")
#         if id_attribute:
#             # Extract the id number from the id attribute
#             id_number = id_attribute.split("anchor")[1]
#             id_numbers.append(id_number)



#     # print("ID numbers:", id_numbers)
#     id_numbers = []
#     modified_xpaths = []
#     # Print XPath of each element
#     for index, element in enumerate(elements, start=1):
#         xpath = driver.execute_script("function getPathTo(element) {\
#                                   if (element.id!=='') return 'id(\"'+element.id+'\")';\
#                                   if (element===document.body) return element.tagName;\
#                                   var ix= 0;\
#                                   var siblings= element.parentNode.childNodes;\
#                                   for (var i= 0; i<siblings.length; i++) {\
#                                     var sibling= siblings[i];\
#                                     if (sibling===element) return getPathTo(element.parentNode)+'/'+element.tagName+'['+(ix+1)+']';\
#                                     if (sibling.nodeType===1 && sibling.tagName===element.tagName) ix++;\
#                                   }\
#                                 } return getPathTo(arguments[0]);", element)
#         modified_xpath = f"//*[@{xpath}"  # Modify XPath string
#         match = re.search(r'anchor(\d+)', modified_xpath)
#         if match:
#             id_numbers.append(match.group(1))
#         modified_xpaths.append(modified_xpath)  # Append modified XPath to list
#         # print(f"//*[@{xpath}")

#     for index, xpath in enumerate(modified_xpaths, start=1):
#         element = driver.find_element(By.XPATH, modify_xpath1(xpath)).text
#         reviews_data.append(element)
#         # print(element)
#         # print(f"Element {index} found using XPath: {modify_xpath1(xpath)}")

#     elements = []
#     for  id_number in id_numbers:
#         found = False
#         for number in range(13, 0, -1):
#             xpath = f'//*[@id="anchor{id_number}"]/div/div[2]/table/tbody/tr[{number}]/td[2]'
#             # print(xpath)
#             try:
#                 element = driver.find_element(By.XPATH,xpath).text
#                 elements.append(element)
#                 recomend_data.append(element)
#                 found = True
#                 break  # Exit the inner loop once an element is found
#             except:
#                 pass
# # Close the browser
# # driver.quit()



"""# **Sentiment Analysis**:

## Data preprocessing
"""

# import os
# import pandas as pd
# from sklearn.metrics import roc_curve, auc

# X_train = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/files/american_airlines_reviews.xlsx")
# y_train = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/files/american_airlines_reviews_r.xlsx")
# mapping = {'yes': 1, 'no': 0}
# y_train['recomend'] = y_train['recomend'].map(mapping)


# print(X_train.head())
# print(y_train.head())

import os
import pandas as pd
from sklearn.metrics import roc_curve, auc


df = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/files/aa1.xlsx")
X_train = df[['review']]
y_train = df[['recommendation']]

mapping = {'yes': 1, 'no': 0}
y_train['recommendation'] = y_train['recommendation'].map(mapping)


print(X_train.head())
print(y_train.head())

##### Data exploration
# We check if the training dataset is balanced.
y_train.value_counts()

print(X_train.head())
print(y_train.head())

from sklearn.utils import resample
import pandas as pd

# df_train = np.column_stack((X_train, y_train))
df_train = pd.concat([X_train, y_train], axis=1)



#create two different dataframe of majority and minority class
df_majority = df_train[(df_train['recommendation']==0)]
df_minority = df_train[(df_train['recommendation']==1)]
# upsample minority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,    # sample with replacement
                                 n_samples= 5110, # to match majority class
                                 random_state=42)  # reproducible results
# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_minority_upsampled, df_majority])

X_train = pd.DataFrame(df_upsampled['review'])
y_train = pd.DataFrame(df_upsampled['recommendation'])
y_train.value_counts()

type(y_train)

###############################
##### Data pre-processing #####
###############################

import nltk
import numpy as np
import matplotlib.pyplot as plt

# Import list of stopwords from library NLTK
from nltk.corpus import stopwords
nltk.download('stopwords')

############################
##### Remove Stopwords #####
############################

stopwords_list = stopwords.words("english")
print(f'List of stopwords:\n{stopwords_list}\n')


# We remove negation words in list of stopwords
# no_stopwords = []
no_stopwords = ["not","don't",'aren','don','ain',"aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't",
               'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't",
               "won't", 'wouldn', "wouldn't"]
for no_stopword in no_stopwords:
    stopwords_list.remove(no_stopword)

print(f'Final list of stopwords:\n{stopwords_list}')

#############################
##### Lemmatize reviews #####
#############################

# Import Lemmatizer from NLTK
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# function that receive a list of words and do lemmatization:
def lemma_stem_text(words_list):
    # Lemmatizer
    text = [lemmatizer.lemmatize(token.lower()) for token in words_list] # eighties->eight or messages->message or drugs->drug
    text = [lemmatizer.lemmatize(token.lower(), "v") for token in text ]# going-> go or started->start or watching->watch
    return text

word_example = "feet"
print(f'The word "{word_example}" is transformed to "{lemma_stem_text([word_example])[0]}"')

##########################################
##### Handling Negative Contractions #####
##########################################

# create a function to change negative contractions (e.g. isn't, can't, ... etc.) to standard from using a regular expression.

import re
re_negation = re.compile("n't ") # specify a pattern you want to find in a string

# function that receive a sequence of words and return the same sequence transforming
# abbreviated negations to the standard form.
def negation_abbreviated_to_standard(sent):
    sent = re_negation.sub(" not ", sent)
    return sent

word_example = "I aren't "
print(f'The sentence "{word_example}" is transformed to "{negation_abbreviated_to_standard(word_example)}"')

###########################
##### Review Cleaning #####
###########################

# create a function to clean the text of a review using the functions defined previously.

# Import function BeautifulSoup to clean text of HTML tags
from bs4 import BeautifulSoup
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')

def review_to_words(raw_review):
    # 1. Remove HTML tags
    review_text = BeautifulSoup(raw_review).get_text()

    # 2. Transform abbreviated negations to the standard form.
    review_text = negation_abbreviated_to_standard(review_text)

    # 3. Remove non-letters and non-numbers
    tokenizer = RegexpTokenizer(r'\w+')  #r'\w+'
    words = tokenizer.tokenize(review_text)

    # 4. Remove stop words
    meaningful_words = [w for w in words if w.lower() not in stopwords_list]

    # 5. Apply lemmatization function
    lemma_words = lemma_stem_text(meaningful_words)

    # 6. Join the words back into one string separated by space, and return the result.
    return( " ".join(lemma_words))

# # Clean first review
# clean_review = review_to_words(X_train["Review"][0] )

# # Print original review, sentiment and cleaned review
# print(f'Text of original review:\n{X_train["Review"][0]}\n')
# # print(f'Sentiment review: {y_train[0]}\n')
# print(f'Text of cleaned review:\n{clean_review}')

#################################
##### Clean for ALL reviews #####
#################################

# We get the text of reviews in the training set
reviews = X_train['review']

# We initialize an empty list to add the clean reviews
cleaned_train_reviews = []

# We loop over each review and clean it
for i in reviews:
    cleaned_train_reviews.append(review_to_words(i))
y_train_old = y_train
# about 1 min

"""## Vectorization with TF-IDF"""

###############################################
##### Vectorization: Create TF-IDF matrix #####
###############################################

# Import tf-idf encoding from sklearn library
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Define some hyperparameters of encoded
#vectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,2))
#vectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,1)) #<---- tf-idf unigram
#vectorizer = CountVectorizer(max_features=20000, ngram_range = (1,1)) #<---- DTM ngram
#vectorizer = CountVectorizer(max_features=20000, ngram_range = (1,1)) #<---- DTM
vectorizer = TfidfVectorizer(max_features=200000, ngram_range = (1,7),smooth_idf=True,norm='l2') #<---- tf-idf 5gram
#vectorizer = TfidfVectorizer(max_features=200000, ngram_range = (1,5)) #<---- tf-idf 5gram #9020
# Create the training set with the words encoded as features of the reviews
train_data_features = vectorizer.fit_transform(cleaned_train_reviews)

# Q2
#vectorizer = CountVectorizer(max_features=20000, ngram_range = (1,1)) #<---- unigram

print(train_data_features.shape)

# y_train_old = y_train
print(len(y_train_old))

"""print(len(y_train_old))"""



#test

#output

"""# **Evaluating the model**

  
   create validation set and evaluate the model.

Word2vec to predict
"""



"""# Sentiment analysis with TF-IDF

# with only logistics regression
"""



# We split train dataset
y_train = y_train_old
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(train_data_features, y_train, test_size=0.2, random_state=42)

len(y_train), len(y_test)

# Initialize a logistic regression model
from sklearn.linear_model import LogisticRegression
logistic = LogisticRegression(random_state=777)
# Train the model
logistic = logistic.fit(X_train, y_train)
# Print score of model(using test dataset)
print(logistic.score(X_test, y_test)) # accuracy
#8944
#9020

y_pred = logistic.predict(X_test)
y_pred_prob = logistic.predict_proba(X_test)[:, 1]

# Calculate accuracy
tf_log_accuracy = accuracy_score(y_test, y_pred)
print("w2v_log_accuracy:", tf_log_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
tf_log_roc_auc = auc(fpr, tpr)
print("ROC AUC:", tf_log_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % tf_log_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""# Sentiment analysis with Word2Vec

# with 1) Logistics, 2)Random Forest, 3) GradientBoost Tree, 4) LASSO, 5) RIDGE

word2vec start from here
"""

# Import necessary libraries
import gensim.downloader as api
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np
filen = 'GoogleNews-vectors-negative300.bin'
bin_file ='/content/drive/MyDrive/Colab Notebooks/files/' + filen # change file path if needed

# Step 1: Load pre-trained Word2Vec model
#word2vec_model = api.load("word2vec-google-news-300")
# Load the Word2Vec model
word2vec_model = KeyedVectors.load_word2vec_format(bin_file, binary=True)

# Step 2: Prepare your text data and labels
# Assuming you have your text data stored in a list called 'corpus'
# and corresponding sentiment labels stored in a list called 'labels'
# train_data_features = cleaned_train_reviews
y_train = y_train_old
# Step 3: Vectorize Text Data
# Define a function to convert text data to word embeddings
def document_vector(word2vec_model, doc):
    """
    Take the mean of word vectors in a document
    """
    doc = [word for word in doc if word in word2vec_model.key_to_index]
    return np.mean(word2vec_model[doc], axis=0)

# Create document vectors for each document in the corpus
X = [document_vector(word2vec_model, doc) for doc in cleaned_train_reviews]

# from sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer

# scaler_standard = StandardScaler()
# X_standardized = scaler_standard.fit_transform(X)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Step 4: Sentiment Analysis Model
# Assuming you have sentiment labels stored in a list called 'labels'
X_train, X_test, y_train, y_test = train_test_split(X, y_train, test_size=0.2, random_state=42)

# Train a logistic regression classifier
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
w2v_log_accuracy = accuracy_score(y_test, y_pred)
print("w2v_log_accuracy:", w2v_log_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
w2v_log_roc_auc = auc(fpr, tpr)
print("ROC AUC:", w2v_log_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % w2v_log_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Train a logistic regression classifier
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
w2v_rf_accuracy = accuracy_score(y_test, y_pred)
print("w2v_rf_accuracy:", w2v_rf_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
w2v_rf_roc_auc = auc(fpr, tpr)
print("ROC AUC:", w2v_rf_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % w2v_rf_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import GradientBoostingClassifier

# Train a logistic regression classifier
classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
w2v_gb_accuracy = accuracy_score(y_test, y_pred)
print("w2v_gb_accuracy:", w2v_gb_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
w2v_gb_roc_auc = auc(fpr, tpr)
print("ROC AUC:", w2v_gb_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % w2v_gb_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification



# Build the Lasso classifier
lasso_classifier = LogisticRegression(penalty='l1', solver='liblinear', C=1 ,random_state=42)

# Train the classifier
lasso_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = lasso_classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
w2v_lasso_accuracy = accuracy_score(y_test, y_pred)
print("w2v_lasso_accuracy:", w2v_lasso_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
w2v_lasso_roc_auc = auc(fpr, tpr)
print("ROC AUC:", w2v_lasso_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % w2v_lasso_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification



# Build the Ridge classifier
ridge_classifier = LogisticRegression(penalty='l2', solver='liblinear', C=1 ,random_state=42)

# Train the classifier
ridge_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = ridge_classifier.predict(X_test)
y_pred_prob = ridge_classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
w2v_ridge_accuracy = accuracy_score(y_test, y_pred)
print("w2v_ridge_accuracy:", w2v_ridge_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
w2v_ridge_roc_auc = auc(fpr, tpr)
print("ROC AUC:", w2v_ridge_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % w2v_ridge_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)
ridge_classifier.coef_

"""# Sentiment analysis with Glove

# with 1) Logistics, 2)Random Forest, 3) GradientBoost Tree, 4) LASSO, 5) RIDGE

glove starts here
"""

from gensim.test.utils import get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import pandas as pd
path = '/content/drive/MyDrive/Colab Notebooks/files/glove/glove.6B.300d.txt' # change!
# locate the temporary path to put converted glove vectors
glove_tmp_path  = get_tmpfile("/content/drive/MyDrive/Colab Notebooks/files/glove/glove.6B.300d.word2vec.txt") # change!
glove2word2vec(path, glove_tmp_path)   #output a (400001, 300) matrix
model = KeyedVectors.load_word2vec_format(glove_tmp_path )

# about 4 mins

# Import necessary libraries
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors

# train_data_features = cleaned_train_reviews
y_train = y_train_old

# Step 2: Prepare your text data and labels
# Assuming you have your text data stored in a list called 'corpus'
# and corresponding sentiment labels stored in a list called 'labels'

# Step 3: Vectorize Text Data
def document_vector(model, doc):
    """
    Take the mean of word vectors in a document
    """
    doc = [word for word in doc if word in model.key_to_index]
    return np.mean([model.get_vector(word) for word in doc], axis=0)



# Create document vectors for each document in the corpus
X = [document_vector(model, doc) for doc in cleaned_train_reviews]

# Step 4: Sentiment Analysis Model
# Assuming you have sentiment labels stored in a list called 'labels'
X_train, X_test, y_train, y_test = train_test_split(X, y_train, test_size=0.2, random_state=42)

# Train a logistic regression classifier
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
glo_log_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_log_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
glo_log_roc_auc = auc(fpr, tpr)
print("ROC AUC:", glo_log_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % glo_log_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Train a logistic regression classifier
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
glo_rf_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_rf_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
glo_rf_roc_auc = auc(fpr, tpr)
print("ROC AUC:", glo_rf_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % glo_rf_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import GradientBoostingClassifier

# Train a logistic regression classifier
classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]


# Calculate accuracy
glo_gb_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_gb_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
glo_gb_roc_auc = auc(fpr, tpr)
print("ROC AUC:", glo_gb_roc_auc)
print(np.unique(y_pred))
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % glo_gb_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification



# Build the Lasso classifier
lasso_classifier = LogisticRegression(penalty='l1', solver='liblinear', C=1 ,random_state=42)

# Train the classifier
lasso_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = lasso_classifier.predict(X_test)
y_pred_prob = lasso_classifier.predict_proba(X_test)[:, 1]
# Calculate accuracy
glo_lasso_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_lasso_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
glo_lasso_roc_auc = auc(fpr, tpr)
print("ROC AUC:", glo_lasso_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % glo_lasso_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification



# Build the ridge classifier
ridge_classifier = LogisticRegression(penalty='l2', solver='liblinear', C=1 ,random_state=42)

# Train the classifier
ridge_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = ridge_classifier.predict(X_test)
y_pred_prob = lasso_classifier.predict_proba(X_test)[:, 1]
# Calculate accuracy
glo_ridge_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_ridge_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
glo_ridge_roc_auc = auc(fpr, tpr)
print("ROC AUC:", glo_ridge_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % glo_ridge_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)
ridge_classifier.coef_

"""Fasttext

"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from gensim.models import FastText
from nltk.tokenize import word_tokenize
from gensim.models import KeyedVectors
import gensim.downloader as api

# Download the pre-trained FastText model
model = api.load('fasttext-wiki-news-subwords-300')

# Save the model for later use
model.save('fasttext_model.bin')

# # Load FastText model
# model = FastText.load_fasttext_format('/content/drive/MyDrive/Colab Notebooks/files/wiki-news-300d-1M.vec')

# train_data_features = cleaned_train_reviews
y_train = y_train_old
# cleaned_train_reviews_df = pd.DataFrame(cleaned_train_reviews)
X = cleaned_train_reviews

# Map tokens to FastText embeddings
def map_tokens_to_embeddings(tokens):
    embeddings = []
    for token in tokens:
        if token in model:
            embeddings.append(model[token])
    if embeddings:
        return np.mean(embeddings, axis=0)
    else:
        return np.zeros(model.vector_size)

X_embeddings = np.array([map_tokens_to_embeddings(tokens) for tokens in X])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_train, test_size=0.2, random_state=42)

# Build logistic regression model
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Evaluate model
ft_log_accuracy = lr_model.score(X_test, y_test)
print("Accuracy:", ft_log_accuracy)

# Make predictions on the test data
y_pred = lr_model.predict(X_test)
y_pred_prob = lr_model.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
# Calculate AUC
np.unique(y_pred)
ft_log_roc_auc = auc(fpr, tpr)
print("ROC AUC:", ft_log_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % ft_log_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Train a logistic regression classifier
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]


# Calculate accuracy
ft_rf_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_rf_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
ft_rf_roc_auc = auc(fpr, tpr)
print("ROC AUC:", ft_rf_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % ft_rf_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import GradientBoostingClassifier

# Train a logistic regression classifier
classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
# Step 5: Evaluation
# Predict sentiment labels for test data
y_pred = classifier.predict(X_test)
y_pred_prob = classifier.predict_proba(X_test)[:, 1]


# Calculate accuracy
ft_gb_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_gb_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
ft_gb_roc_auc = auc(fpr, tpr)
print("ROC AUC:", ft_gb_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % ft_gb_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification



# Build the Lasso classifier
lasso_classifier = LogisticRegression(penalty='l1', solver='liblinear', C=1 ,random_state=42)

# Train the classifier
lasso_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = lasso_classifier.predict(X_test)
y_pred_prob = lasso_classifier.predict_proba(X_test)[:, 1]


# Calculate accuracy
ft_lasso_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_lasso_accuracy)


# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
ft_lasso_roc_auc = auc(fpr, tpr)
print("ROC AUC:", ft_lasso_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % ft_lasso_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification



# Build the ridge classifier
ridge_classifier = LogisticRegression(penalty='l2', solver='liblinear', C=1 ,random_state=42)

# Train the classifier
ridge_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = ridge_classifier.predict(X_test)
y_pred_prob = ridge_classifier.predict_proba(X_test)[:, 1]

# Calculate accuracy
ft_ridge_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", glo_ridge_accuracy)

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate AUC
np.unique(y_pred)
ft_ridge_roc_auc = auc(fpr, tpr)
print("ROC AUC:", ft_ridge_roc_auc)
print(np.unique(y_pred))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % ft_ridge_roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

np.unique(y_pred)

from sklearn.metrics import roc_curve, auc


# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred)

# Calculate AUC
roc_auc = auc(fpr, tpr)
print("ROC AUC:", roc_auc)

print(
w2v_log_accuracy,
w2v_log_roc_auc,
w2v_lasso_accuracy,
w2v_lasso_roc_auc,
w2v_ridge_accuracy,
w2v_ridge_roc_auc,
w2v_rf_accuracy,
w2v_rf_roc_auc,
w2v_gb_accuracy,
w2v_gb_roc_auc)

print(
glo_log_accuracy,
glo_log_roc_auc,
glo_lasso_accuracy,
glo_lasso_roc_auc,
glo_ridge_accuracy,
glo_ridge_roc_auc,
glo_rf_accuracy,
glo_rf_roc_auc,
glo_gb_accuracy,
glo_gb_roc_auc)

print(
ft_log_accuracy,
ft_log_roc_auc,
ft_lasso_accuracy,
ft_lasso_roc_auc,
ft_ridge_accuracy,
ft_ridge_roc_auc,
ft_rf_accuracy,
ft_rf_roc_auc,
ft_gb_accuracy,
ft_gb_roc_auc)

!pip install pandas openpyxl
import pandas as pd

# Data
data = {
    "Model": [
        "Logistics", "LASSO", "RIDGE", "Random Forest", "Boosted Tree"
    ],
    "Accuracy": [
        ft_log_accuracy, ft_lasso_accuracy, ft_ridge_accuracy, ft_rf_accuracy, ft_gb_accuracy
    ],
    "ROC_AUC": [
        ft_log_roc_auc, ft_lasso_roc_auc, ft_ridge_roc_auc, ft_rf_roc_auc, ft_gb_roc_auc
    ]
}

# Create DataFrame
df = pd.DataFrame(data)

# Save to Excel
df.to_excel("/content/drive/MyDrive/Colab Notebooks/files/aa_model__over_results2.xlsx", index=False)

print(
w2v_log_accuracy,
w2v_log_roc_auc,
w2v_lasso_accuracy,
w2v_lasso_roc_auc,
w2v_ridge_accuracy,
w2v_ridge_roc_auc,
w2v_rf_accuracy,
w2v_rf_roc_auc,
w2v_gb_accuracy,
w2v_gb_roc_auc)

print(
ft_log_accuracy,
ft_log_roc_auc,
ft_lasso_accuracy,
ft_lasso_roc_auc,
ft_ridge_accuracy,
ft_ridge_roc_auc,
ft_rf_accuracy,
ft_rf_roc_auc,
ft_gb_accuracy,
ft_gb_roc_auc)

print(ft_log_accuracy,
ft_lasso_accuracy,
ft_ridge_accuracy,
ft_rf_accuracy,
ft_gb_accuracy)

print(ft_log_roc_auc,
ft_lasso_roc_auc,
ft_ridge_roc_auc,
ft_rf_roc_auc,
ft_gb_roc_auc)



# !pip install pandas openpyxl
import pandas as pd

# Data
data = {
    "Model": [
        "w2v_Logistics", "w2v_LASSO", "w2v_RIDGE", "w2v_Random Forest", "w2v_Boosted Tree",
        "glo_Logistics", "glo_LASSO", "glo_RIDGE", "glo_Random Forest", "glo_Boosted Tree",
        "ft_Logistics", "ft_LASSO", "ft_RIDGE", "ft_Random Forest", "ft_Boosted Tree"
    ],
    "Accuracy": [
        w2v_log_accuracy, w2v_lasso_accuracy, w2v_ridge_accuracy, w2v_rf_accuracy, w2v_gb_accuracy,
        glo_log_accuracy, glo_lasso_accuracy, glo_ridge_accuracy, glo_rf_accuracy, glo_gb_accuracy,
        ft_log_accuracy, ft_lasso_accuracy, ft_ridge_accuracy, ft_rf_accuracy, ft_gb_accuracy
    ],
    "ROC_AUC": [
        w2v_log_roc_auc, w2v_lasso_roc_auc, w2v_ridge_roc_auc, w2v_rf_roc_auc, w2v_gb_roc_auc,
        glo_log_roc_auc, glo_lasso_roc_auc, glo_ridge_roc_auc, glo_rf_roc_auc, glo_gb_roc_auc,
        ft_log_roc_auc, ft_lasso_roc_auc, ft_ridge_roc_auc, ft_rf_roc_auc, ft_gb_roc_auc

    ]
}

# Create DataFrame
df = pd.DataFrame(data)

# Save to Excel
df.to_excel("/content/drive/MyDrive/Colab Notebooks/files/aa_model_results_over_full.xlsx", index=False)