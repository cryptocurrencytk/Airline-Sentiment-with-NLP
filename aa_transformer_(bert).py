# -*- coding: utf-8 -*-
"""AA- Transformer (BERT).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wc4D_etEuVSBCHZSAe_SUYKqnaBjh2Q0
"""

import pandas as pd
from transformers import BertTokenizer

# Mounted content
from google.colab import drive
drive.mount('/content/drive')

# Load file
df = pd.read_excel("/content/drive/MyDrive/Final-653/aa1.xlsx")
df["recommendation"]=df["recommendation"].apply(lambda x: 1 if 'yes' in x else 0)
df.head(5)

df["recommendation"].value_counts()

"""
# Bert

"""

import torch

# check GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# Import package
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim

# Split data
features_list= df["review"].tolist()
labels_list=  df["recommendation"].tolist()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features_list,labels_list, test_size=0.2, random_state=42)
#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2

"""

# Tokenizing"""

tokenizer= BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize Train reviews

inputs= tokenizer(X_train,max_length = 256, truncation = True, padding = True, return_tensors= "pt")

# Tokenize labels

labels = torch.tensor(y_train)

labels

inputs

inputs['input_ids'].shape

inputs['attention_mask'].shape

# Test document tokenizing
inputs_test = tokenizer(X_test, max_length= 256, truncation = True, padding = True, return_tensors= "pt")

# Transform the test label to Tensor object
labels_test= torch.tensor(y_test)

labels_test.shape

"""
# TensorDataset & DataLoader

"""

train_dataset= TensorDataset (inputs.input_ids, inputs.attention_mask, labels)

len(train_dataset)

test_dataset= TensorDataset (inputs_test.input_ids, inputs_test.attention_mask, labels_test)
len(test_dataset)

train_loader = DataLoader (train_dataset, batch_size = 32,shuffle = True)
len(train_loader)

test_loader = DataLoader (test_dataset, batch_size = 32,shuffle = True)
len(test_loader)

"""

# Load Pre_trained BERT model

"""

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels = 2)

model.to(device)

"""# Train"""

optimizer = optim.AdamW(model.parameters(), lr= 1e-5)
loss_fn = torch.nn.CrossEntropyLoss()

epochs=4
print_every_n=100

model.train()

for epoch in range(epochs):
  running_loss = 0.0
  for i, batch in enumerate(train_loader):
    optimizer.zero_grad()
    # inputs, labels --> send to GPU
    input_ids, attention_mask, labels = [b.to(device) for b in batch]
    outputs = model(input_ids, attention_mask = attention_mask)
    loss = loss_fn(outputs.logits, labels)
    loss.backward()
    optimizer.step()

    running_loss += loss.item() # getting just float number from Tensor

    # checking loss
    if (i % print_every_n == 0) and (i > 0):
      average_loss = running_loss / print_every_n
      print(f'Epoch {epoch + 1}', '\t', f'Batch {i}','\t', f'Average Loss: {average_loss:.4f}')
      running_loss = 0.0

  print(f'Epoch {epoch + 1} finished.')

"""# Evaluation on Test set"""

model.eval() # freeze updated model parameters

total_correct = 0
total_count = 0
pred_probs = []
true_labels = []
with torch.no_grad():
  for i, batch in enumerate(test_loader): # test_loader !
    input_ids, attention_mask, labels = [b.to(device) for b in batch]
    outputs = model(input_ids, attention_mask = attention_mask)
    logits = outputs.logits # (batch_size, n_label) \hat{y}
    if i ==0:
      print(logits)
      print(logits.shape)
    predictions = torch.argmax(logits, dim = 1)
    correct = (predictions == labels).sum().item()
    total_correct += correct
    total_count += labels.size(0)
    # Get the probability of the positive class ("recommended")

    probs = torch.softmax(logits, dim=1)
    pred_probs.extend(probs[:, 1].tolist())
    true_labels.extend(labels.tolist())

accuracy = total_correct / total_count * 100
print(f'Test accuracy: {accuracy:.2f} %')

from sklearn.metrics import roc_auc_score, f1_score
# Calculate AUC score
auc_score = roc_auc_score(true_labels, pred_probs)

# Convert predicted probabilities to labels
pred_labels = [1 if prob >= 0.5 else 0 for prob in pred_probs]

# Calculate F1 score
f1 = f1_score(true_labels, pred_labels)

print("AUC Score:", auc_score)
print("F1 Score:", f1)

total_count

"""# Prediction"""

def predict(sentence:str):
  # tokenize
  inputs = tokenizer(sentence, max_length = 512, padding = True, truncation = True, return_tensors = 'pt')
  input_ids = inputs['input_ids'].to(device)
  attention_mask = inputs['attention_mask'].to(device)

  # forward
  outputs = model(input_ids,  attention_mask = attention_mask)
  prediction = torch.argmax(outputs.logits, dim = 1).item()

  return prediction

# sentence = 'My adorable dog is so lovely'
sentence = "The service is great"
predict(sentence)

sentence1 = "The flight delay for almost 3 hours"
predict(sentence1)

sentence2 = "they try to give us a coupon for a 2 hour delay flight. Money can't buy our time."
predict(sentence2)

sentence3 = " The flight to NY delay for 3 hours, AA give us a $500 coupon as their compromise. It seems ok for me"
predict(sentence3)